{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12944,"sourceType":"datasetVersion","datasetId":9235},{"sourceId":1526618,"sourceType":"datasetVersion","datasetId":897156},{"sourceId":8531859,"sourceType":"datasetVersion","datasetId":5095690}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom gensim.models import KeyedVectors\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import normalize\n\nnltk.data.path.append('/usr/share/nltk_data/')\nnltk.download('punkt', force=True)\nnltk.download('stopwords', force=True)\nnltk.download('wordnet', force=True)\n\n! cp - rf / usr/share/nltk_data/corpora/wordnet  # temp fix for lookup error.\n\n!unzip -o /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\n# Load dataset\ndata_path = '/kaggle/input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv'\ndata = pd.read_csv(data_path)\n\n# Preprocessing: Text Cleaning, Tokenization, and Normalization\ndef clean_text(text):\n    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text.lower()\n\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    text = clean_text(text)\n    tokens = nltk.word_tokenize(text)\n    tokens = [lemmatizer.lemmatize(word)\n              for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\ndata['Cleaned_Review'] = data['Review'].apply(preprocess_text)\n\n# Adjust labels for sentiment analysis\ndata['Sentiment'] = data['Rating'].apply(lambda x: 'negative' if x < 3 else ('neutral' if x == 3 else 'positive'))\n\n# Convert 'Sentiment' column to category\ndata['Sentiment'] = data['Sentiment'].astype('category')\n\n# Split data into train and test sets (70% train, 30% test)\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    data['Cleaned_Review'], data['Sentiment'], test_size=0.3, random_state=42)\n\n# Print total sentiment in train and test set\nprint(\"\\nTotal sentiment in train set:\")\nprint(train_labels.value_counts())\n\nprint(\"\\nTotal sentiment in test set:\")\nprint(test_labels.value_counts())\n\n# Load FastText model\nfasttext_model_path = '/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec'\nfasttext_vectors = KeyedVectors.load_word2vec_format(fasttext_model_path, binary=False)\n\n# Convert text data to FastText vectors (average of word vectors)\ndef text_to_vector(text):\n    tokens = nltk.word_tokenize(text.lower())\n    vectors = [fasttext_vectors[word] for word in tokens if word in fasttext_vectors]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n\ntrain_vectors = np.array([text_to_vector(text) for text in train_texts])\ntest_vectors = np.array([text_to_vector(text) for text in test_texts])\n\n# Normalize vectors\ntrain_vectors = normalize(train_vectors, axis=1)\ntest_vectors = normalize(test_vectors, axis=1)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\ntrain_vectors_smote, train_labels_smote = smote.fit_resample(train_vectors, train_labels)\n\ntrain_vectors, train_labels = train_vectors_smote, train_labels_smote\n\n# Apply ADASYN to the training data\n# adasyn = ADASYN(random_state=42)\n# train_vectors_adasyn, train_labels_adasyn = adasyn.fit_resample(train_vectors, train_labels)\n\n# Concatenate the results from SMote and ADASYN\n# train_vectors = np.concatenate([train_vectors_smote, train_vectors_adasyn])\n# train_labels = np.concatenate([train_labels_smote, train_labels_adasyn])\n\n# Convert the numpy arrays back to pandas Series and ensure they are of type category\ntrain_labels = pd.Series(train_labels, dtype=\"category\")\ntest_labels = pd.Series(test_labels, dtype=\"category\")\n\n# Optionally, you can check the distribution of the new training labels\nprint(\"Combined training label distribution:\")\nprint(Counter(train_labels))\n\n# Calculate class weights\nclass_counts = Counter(train_labels)\ntotal_samples = len(train_labels)\nclass_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n\n# Convert class weights to tensor\nclass_weights_tensor = torch.tensor([class_weights[cls] for cls in sorted(class_counts.keys())], dtype=torch.float32)\n\n# LSTM Model with dropout\nclass SentimentLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.3):\n        super(SentimentLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add dimension for seq_length\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        lstm_out = self.dropout(lstm_out)\n        lstm_out = lstm_out[:, -1, :]\n        output = self.fc(lstm_out)\n        return output\n\n# Define parameters\ninput_size = 300  # Size of FastText word vectors\nhidden_size = 512\noutput_size = len(data['Sentiment'].unique())\n\n# Create LSTM model instance\nlstm_model = SentimentLSTM(input_size, hidden_size, output_size, num_layers=2, dropout=0.3)\n\n# Move class weights to the same device as the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nclass_weights_tensor = class_weights_tensor.to(device)\n\n# Define optimizer and loss function\noptimizer = optim.Adam(lstm_model.parameters(), lr=1e-4)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n\n# Convert data to tensors\ntrain_vectors_tensor = torch.tensor(train_vectors, dtype=torch.float32).to(device)\ntest_vectors_tensor = torch.tensor(test_vectors, dtype=torch.float32).to(device)\ntrain_labels_tensor = torch.tensor(train_labels.cat.codes.values, dtype=torch.long).to(device)\ntest_labels_tensor = torch.tensor(test_labels.cat.codes.values, dtype=torch.long).to(device)\n\n# Define DataLoader\ntrain_dataset = torch.utils.data.TensorDataset(train_vectors_tensor, train_labels_tensor)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Training loop\ndef train_model(model, train_loader, optimizer, loss_fn, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_predictions = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = correct_predictions / len(train_loader.dataset)\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n    return model\n\n# Move model to GPU if available\nlstm_model = lstm_model.to(device)\n\n# Train the LSTM model\ntrained_lstm_model = train_model(lstm_model, train_loader, optimizer, loss_fn, num_epochs=10)\n\n# Evaluate the model\ndef evaluate_model(model, test_vectors_tensor, test_labels_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(test_vectors_tensor)\n        _, predicted = torch.max(outputs, 1)\n        cm = confusion_matrix(test_labels_tensor.cpu(), predicted.cpu())\n        cr = classification_report(test_labels_tensor.cpu(), predicted.cpu(), target_names=['negative', 'neutral', 'positive'])\n    return cm, cr\n\n# Convert test data to tensor\ntest_vectors_tensor = test_vectors_tensor.to(device)\ntest_labels_tensor = test_labels_tensor.to(device)\n\n# Evaluate the LSTM model\nconf_matrix, class_report = evaluate_model(trained_lstm_model, test_vectors_tensor, test_labels_tensor)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nClassification Report:\")\nprint(class_report)\n\n# Confusion Matrix Visualization\ndef plot_confusion_matrix(cm, labels):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.show()\n\nplot_confusion_matrix(conf_matrix, ['negative', 'neutral', 'positive'])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T08:01:37.042129Z","iopub.execute_input":"2024-06-16T08:01:37.042522Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\ncp: target 'usr/share/nltk_data/corpora/wordnet' is not a directory\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]}]}