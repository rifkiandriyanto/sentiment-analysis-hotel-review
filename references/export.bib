@article{,
   author = {G Sri vidhya and R Nagarajan},
   journal = {Computing},
   pages = {1-30},
   publisher = {Springer},
   title = {A novel bidirectional LSTM model for network intrusion detection in SDN-IoT network},
   year = {2024},
}
@article{Ebrahimi2020,
   abstract = {Deep Learning (DL) has recently become a topic of study in different applications including healthcare, in which timely detection of anomalies on Electrocardiogram (ECG) can play a vital role in patient monitoring. This paper presents a comprehensive review study on the recent DL methods applied to the ECG signal for the classification purposes. This study considers various types of the DL methods such as Convolutional Neural Network (CNN), Deep Belief Network (DBN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). From the 75 studies reported within 2017 and 2018, CNN is dominantly observed as the suitable technique for feature extraction, seen in 52% of the studies. DL methods showed high accuracy in correct classification of Atrial Fibrillation (AF) (100%), Supraventricular Ectopic Beats (SVEB) (99.8%), and Ventricular Ectopic Beats (VEB) (99.7%) using the GRU/LSTM, CNN, and LSTM, respectively.},
   author = {Zahra Ebrahimi and Mohammad Loni and Masoud Daneshtalab and Arash Gharehbaghi},
   doi = {10.1016/J.ESWAX.2020.100033},
   issn = {2590-1885},
   journal = {Expert Systems with Applications: X},
   keywords = {Computer-Aided Diagnosis,Deep Learning,Electrocardiogram,Smart health-care},
   month = {9},
   pages = {100033},
   publisher = {Elsevier},
   title = {A review on deep learning methods for ECG arrhythmia classification},
   volume = {7},
   year = {2020},
}
@inproceedings{Mikolov2013,
   author = {Tomáš Mikolov and Wen-tau Yih and Geoffrey Zweig},
   booktitle = {Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
   pages = {746-751},
   title = {Linguistic regularities in continuous space word representations},
   year = {2013},
}
@article{Kumar2012,
   author = {Akshi Kumar and Mary Sebastian Teeja},
   issue = {10},
   journal = {International Journal of Intelligent Systems and Applications},
   pages = {1},
   publisher = {Citeseer},
   title = {Sentiment analysis: A perspective on its past, present and future},
   volume = {4},
   year = {2012},
}
@article{Sukhbaatar2015,
   author = {Sainbayar Sukhbaatar and Jason Weston and Rob Fergus and others},
   journal = {Advances in neural information processing systems},
   title = {End-to-end memory networks},
   volume = {28},
   year = {2015},
}
@article{Wang2021,
   author = {Kun Wang and Jun He and Lei Zhang},
   issue = {4},
   journal = {IEEE Transactions on Human-Machine Systems},
   pages = {355-364},
   publisher = {IEEE},
   title = {Sequential weakly labeled multiactivity localization and recognition on wearable sensors using recurrent attention networks},
   volume = {51},
   year = {2021},
}
@inproceedings{Graves2013,
   author = {Alex Graves and Abdel-rahman Mohamed and Geoffrey Hinton},
   booktitle = {2013 IEEE international conference on acoustics, speech and signal processing},
   pages = {6645-6649},
   title = {Speech recognition with deep recurrent neural networks},
   year = {2013},
}
@article{Schmidhuber2015,
   author = {Jürgen Schmidhuber},
   journal = {Neural networks},
   pages = {85-117},
   publisher = {Elsevier},
   title = {Deep learning in neural networks: An overview},
   volume = {61},
   year = {2015},
}
@article{Ma2016,
   author = {Xuezhe Ma and Eduard H Hovy},
   journal = {CoRR},
   title = {End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
   volume = {abs/1603.01354},
   url = {http://arxiv.org/abs/1603.01354},
   year = {2016},
}
@inproceedings{Kohavi1995,
   author = {Ron Kohavi and others},
   issue = {2},
   booktitle = {Ijcai},
   pages = {1137-1145},
   title = {A study of cross-validation and bootstrap for accuracy estimation and model selection},
   volume = {14},
   year = {1995},
}
@article{Hochreiter1997,
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   issue = {8},
   journal = {Neural computation},
   pages = {1735-1780},
   publisher = {MIT press},
   title = {Long short-term memory},
   volume = {9},
   year = {1997},
}
@article{Schuster1997,
   author = {Mike Schuster and Kuldip K Paliwal},
   issue = {11},
   journal = {IEEE transactions on Signal Processing},
   pages = {2673-2681},
   publisher = {Ieee},
   title = {Bidirectional recurrent neural networks},
   volume = {45},
   year = {1997},
}
@book{Jurafsky2000,
   author = {Dan Jurafsky},
   publisher = {Pearson Education India},
   title = {Speech & language processing},
   year = {2000},
}
@article{Chawla2002,
   author = {Nitesh V Chawla and Kevin W Bowyer and Lawrence O Hall and W Philip Kegelmeyer},
   journal = {Journal of artificial intelligence research},
   pages = {321-357},
   title = {SMOTE: synthetic minority over-sampling technique},
   volume = {16},
   year = {2002},
}
@article{Japkowicz2002,
   author = {Nathalie Japkowicz and Shaju Stephen},
   issue = {5},
   journal = {Intelligent data analysis},
   pages = {429-449},
   publisher = {IOS Press},
   title = {The class imbalance problem: A systematic study},
   volume = {6},
   year = {2002},
}
@inproceedings{Morin2005,
   author = {Frederic Morin and Yoshua Bengio},
   booktitle = {International workshop on artificial intelligence and statistics},
   pages = {246-252},
   title = {Hierarchical probabilistic neural network language model},
   year = {2005},
}
@article{Graves2005,
   abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. © 2005 Elsevier Ltd. All rights reserved.},
   author = {Alex Graves and Jürgen Schmidhuber},
   doi = {10.1016/J.NEUNET.2005.06.042},
   issn = {0893-6080},
   issue = {5-6},
   journal = {Neural Networks},
   month = {8},
   pages = {602-610},
   pmid = {16112549},
   publisher = {Pergamon},
   title = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
   volume = {18},
   year = {2005},
}
@article{Mikolov2013,
   author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg S Corrado and Jeff Dean},
   journal = {Advances in neural information processing systems},
   title = {Distributed representations of words and phrases and their compositionality},
   volume = {26},
   year = {2013},
}
@article{Bojanowski2016,
   abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
   author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
   month = {8},
   title = {Enriching Word Vectors with Subword Information},
   year = {2016},
}
@inproceedings{Yao2017,
   author = {Xianglu Yao},
   booktitle = {Proc. Int. Conf. Inf. Sci. Cloud Comput},
   pages = {110-117},
   title = {Attention-based BiLSTM neural networks for sentiment classification of short texts},
   year = {2017},
}
@article{Yu2019,
   author = {Yong Yu and Xiaosheng Si and Changhua Hu and Jianxun Zhang},
   issue = {7},
   journal = {Neural computation},
   pages = {1235-1270},
   publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
   title = {A review of recurrent neural networks: LSTM cells and network architectures},
   volume = {31},
   year = {2019},
}
@article{Fahrudin2019,
   author = {Tora Fahrudin and Joko Lianto Buliali and Chastine Fatichah},
   issue = {2},
   journal = {Int. J. Innov. Comput. Inf. Control},
   pages = {423-444},
   title = {Enhancing the performance of smote algorithm by using attribute weighting scheme and new selective sampling method for imbalanced data set},
   volume = {15},
   year = {2019},
}
@article{Dang2020,
   author = {Nhan Cach Dang and Mar\'\ia N Moreno-Garc\'\ia and Fernando la Prieta},
   issue = {3},
   journal = {Electronics},
   pages = {483},
   publisher = {MDPI},
   title = {Sentiment analysis based on deep learning: A comparative study},
   volume = {9},
   year = {2020},
}
@inproceedings{Khotijah2020,
   author = {Siti Khotijah and Jimmy Tirtawangsa and Arie A Suryani},
   booktitle = {Proceedings of the 11th international conference on advances in information technology},
   pages = {1-7},
   title = {Using lstm for context based approach of sarcasm detection in twitter},
   year = {2020},
}
@article{Misra2020,
   author = {Puneet Misra and Arun Singh Yadav},
   issue = {3},
   journal = {Int. J. Emerg. Technol},
   pages = {659-665},
   title = {Improving the classification accuracy using recursive feature elimination with cross-validation},
   volume = {11},
   year = {2020},
}
@article{Gunawan2021,
   author = {Kristian Indradiarta Gunawan and Joan Santoso},
   issue = {01},
   journal = {Journal of Information System, Graphics, Hospitality and Technology},
   pages = {29-38},
   title = {Multilabel text classification menggunakan svm dan doc2vec classification pada dokumen berita bahasa indonesia},
   volume = {3},
   year = {2021},
}
@article{Wu2021,
   abstract = {We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.},
   author = {Peng Wu and Xiaotong Li and Chen Ling and Shengchun Ding and Si Shen},
   doi = {10.1016/J.ASOC.2021.107792},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Attention mechanism,Bidirectional long short-term memory network,Sentiment classification,Social media,Word embedding},
   month = {8},
   pages = {107792},
   publisher = {Elsevier},
   title = {Sentiment classification using attention mechanism and bidirectional long short-term memory network},
   volume = {112},
   year = {2021},
}
@article{Dablain2022,
   author = {Damien Dablain and Bartosz Krawczyk and Nitesh V Chawla},
   issue = {9},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   pages = {6390-6404},
   publisher = {IEEE},
   title = {DeepSMOTE: Fusing deep learning and SMOTE for imbalanced data},
   volume = {34},
   year = {2022},
}
@article{Chandio2022,
   abstract = {Deep neural networks have emerged as a leading approach towards handling many natural language processing (NLP) tasks. Deep networks initially conquered the problems of computer vision. However, dealing with sequential data such as text and sound was a nightmare for such networks as traditional deep networks are not reliable in preserving contextual information. This may not harm the results in the case of image processing where we do not care about the sequence, but when we consider the data collected from text for processing, such networks may trigger disastrous results. Moreover, establishing sentence semantics in a colloquial text such as Roman Urdu is a challenge. Additionally, the sparsity and high dimensionality of data in such informal text have encountered a significant challenge for building sentence semantics. To overcome this problem, we propose a deep recurrent architecture RU-BiLSTM based on bidirectional LSTM (BiLSTM) coupled with word embedding and an attention mechanism for sentiment analysis of Roman Urdu. Our proposed model uses the bidirectional LSTM to preserve the context in both directions and the attention mechanism to concentrate on more important features. Eventually, the last dense softmax output layer is used to acquire the binary and ternary classification results. We empirically evaluated our model on two available datasets of Roman Urdu, i.e., RUECD and RUSA-19. Our proposed model outperformed the baseline models on many grounds, and a significant improvement of 6% to 8% is achieved over baseline models.},
   author = {Bilal Ahmed Chandio and Ali Shariq Imran and Maheen Bakhtyar and Sher Muhammad Daudpota and Junaid Baber},
   doi = {10.3390/app12073641},
   issn = {20763417},
   issue = {7},
   journal = {Applied Sciences (Switzerland)},
   keywords = {LSTM,Roman Urdu,attention networks,deep learning,natural language processing,neural networks,sentiment analysis,text processing},
   month = {8},
   publisher = {MDPI},
   title = {Attention-Based RU-BiLSTM Sentiment Analysis Model for Roman Urdu},
   volume = {12},
   year = {2022},
}
@book{Liu2022,
   author = {Bing Liu},
   publisher = {Springer Nature},
   title = {Sentiment analysis and opinion mining},
   year = {2022},
}
@article{Hamoud2022,
   abstract = {Subjectivity analysis is one of the key tasks in the field of natural language processing. Used to annotate data as subjective or objective, subjectivity analysis can be implemented on its own or as a precursor to other NLP applications such as sentiment analysis, emotion analysis, consumer review analysis, political opinion analysis, document summarization, and question answering systems. The main objective of this article is to test and compare six deep learning methods for subjectivity classification, including Long Short-Term Memory Networks (LSTM), Gated Recurrent Units (GRU), bidirectional GRU, bidirectional LSTM, LSTM with attention, and bidirectional LSTM with attention. We introduced a combination method for subjectivity annotation using lexicon-based and syntactic pattern-based methods. We evaluated the performance of GloVe versus one-hot encoding. We also reformatted, preprocessed, and annotated a political and ideological debate dataset for use in subjectivity analysis. Our research compares favorably with the performance of existing research on subjectivity analysis, achieving very high accuracy and evaluation metrics. LSTM with attention performed the best out of all the methods we tested with an accuracy of 97.39%.},
   author = {Ahmed Al Hamoud and Amber Hoenig and Kaushik Roy},
   doi = {10.1016/j.jksuci.2022.07.014},
   issn = {22131248},
   issue = {10},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   keywords = {Attention network,Deep learning,Gated Recurrent Unit (GRU),LSTM with attention,Long Short-Term Memory (LSTM),Subjectivity analysis},
   month = {8},
   pages = {7974-7987},
   publisher = {King Saud bin Abdulaziz University},
   title = {Sentence subjectivity analysis of a political and ideological debate dataset using LSTM and BiLSTM with attention and GRU models},
   volume = {34},
   year = {2022},
}
@article{Pei2022,
   abstract = {In recent years, more and more attention has been paid to text sentiment analysis, which has gradually become a research hotspot in information extraction, data mining, Natural Language Processing (NLP), and other fields. With the gradual popularization of the Internet, sentiment analysis of Uyghur texts has great research and application value in online public opinion. For low-resource languages, most state-of-the-art systems require tens of thousands of annotated sentences to get high performance. However, there is minimal annotated data available about Uyghur sentiment analysis tasks. There are also specificities in each task—differences in words and word order across languages make it a challenging problem. In this paper, we present an effective solution to providing a meaningful and easy-to-use feature extractor for sentiment analysis tasks: using the pre-trained language model with BiLSTM layer. Firstly, data augmentation is carried out by AEDA (An Easier Data Augmentation), and the augmented dataset is constructed to improve the performance of text classification tasks. Then, a pretraining model LaBSE is used to encode the input data. Then, BiLSTM is used to learn more context information. Finally, the validity of the model is verified via two categories datasets for sentiment analysis and five categories datasets for emotion analysis. We evaluated our approach on two datasets, which showed wonderful performance compared to some strong baselines. We close with an overview of the resources for sentiment analysis tasks and some of the open research questions. Therefore, we propose a combined deep learning and cross-language pretraining model for two low resource expectations.},
   author = {Yijie Pei and Siqi Chen and Zunwang Ke and Wushour Silamu and Qinglang Guo},
   doi = {10.3390/app12031182},
   issn = {20763417},
   issue = {3},
   journal = {Applied Sciences (Switzerland)},
   keywords = {BiLSTM,Cross-lingual pre-trained language model,Data augmentation,Low-resource,Sentiment analysis},
   month = {8},
   publisher = {MDPI},
   title = {AB-LaBSE: Uyghur Sentiment Analysis via the Pre-Training Model with BiLSTM},
   volume = {12},
   year = {2022},
}
@article{Mahesh2023,
   author = {T R Mahesh and Oana Geman and Martin Margala and Manisha Guduri and others},
   journal = {Healthcare Analytics},
   pages = {100247},
   publisher = {Elsevier},
   title = {The stratified K-folds cross-validation and class-balancing methods with high-performance ensemble classifiers for breast cancer classification},
   volume = {4},
   year = {2023},
}
@article{Li2022,
   abstract = {Sentiment analysis of online Chinese buzzwords (OCBs) is important for healthy development of platforms, such as games and social networking, which can avoid transmission of negative emotions through prediction of users’ sentiment tendencies. Buzzwords have the characteristics of varying text length, irregular wording, ignoring syntactic and grammatical requirements, no complete semantic structure, and no obvious sentiment features. This results in interference and challenges to the sentiment analysis of such texts. Sentiment analysis also requires capturing effective sentiment features from deeper contextual information. To solve the above problems, we propose a deep learning model combining BERT and BiLSTM. The goal is to generate dynamic representations of OCB vectors in downstream tasks by fine-tuning the BERT model and to capture the rich information of the text at the embedding layer to solve the problem of static representations of word vectors. The generated word vectors are then transferred to the BiLSTM model for feature extraction to obtain the local and global semantic features of the text while highlighting the text sentiment polarity for sentiment classification. The experimental results show that the model works well in terms of the comprehensive evaluation index F1. Our model also has important significance and research value for sentiment analysis of irregular texts, such as OCBs.},
   author = {Xinlu Li and Yuanyuan Lei and Shengwei Ji},
   doi = {10.3390/fi14110332},
   issn = {19995903},
   issue = {11},
   journal = {Future Internet},
   keywords = {BiLSTM,deep learning,online Chinese buzzwords,pre-trained language models,sentiment analysis},
   month = {8},
   publisher = {MDPI},
   title = {BERT- and BiLSTM-Based Sentiment Analysis of Online Chinese Buzzwords},
   volume = {14},
   year = {2022},
}
@article{Yuan2023,
   abstract = {Product reviews provide crucial information for both consumers and businesses, offering insights needed before purchasing a product or service. However, existing sentiment analysis methods, especially for Chinese language, struggle to effectively capture contextual information due to the complex semantics, multiple sentiment polarities, and long-term dependencies between words. In this paper, we propose a sentiment classification method based on the BiLSTM algorithm to address these challenges in natural language processing. Self-Attention-CNN BiLSTM (SAC-BiLSTM) leverages dual channels to extract features from both character-level embeddings and word-level embeddings. It combines BiLSTM and Self-Attention mechanisms for feature extraction and weight allocation, aiming to overcome the limitations in mining contextual information. Experiments were conducted on the onlineshopping10cats dataset, which is a standard corpus of e-commerce shopping reviews available in the ChineseNlpCorpus 2018. The experimental results demonstrate the effectiveness of our proposed algorithm, with Recall, Precision, and F1 scores reaching 0.9409, 0.9369, and 0.9404, respectively.},
   author = {Ye Yuan and Wang Wang and Guangze Wen and Zikun Zheng and Zhemin Zhuang},
   doi = {10.3390/fi15110364},
   issn = {19995903},
   issue = {11},
   journal = {Future Internet},
   keywords = {BiLSTM,natural language processing,scalable multi-channel,self-attention,sentiment classification},
   month = {8},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Sentiment Analysis of Chinese Product Reviews Based on Fusion of DUAL-Channel BiLSTM and Self-Attention},
   volume = {15},
   year = {2023},
}
@article{Sangeetha2023,
   abstract = {Sentiment analysis can assist consumers in providing clear and objective sentiment recommendations based on large amounts of data, and it is helpful in overcoming unclear human flaws in subjective assessments. Existing sentiment analysis methods, on the other hand, must be enhanced in terms of robustness and accuracy. To improve marketing strategies based on product reviews, a reliable mechanism for forecasting sentiment polarity should be implemented. This paper proposes a new approach for sentiment analysis called Taylor–Harris Hawks Optimization driven long short-term memory (THHO- BiLSTM). By incorporating Taylor series in HHO, Taylor–HHO is formed, which aids in improving the BiLSTM classifier's performance by picking optimal weights in the hidden layers. The proposed method was evaluated using Amazon product reviews and reviews from the Taboada corpus benchmark datasets, yielding findings with 96.93% and 93% accuracy, respectively. When compared to existing approaches, the suggested model exceeds them in terms of accuracy. The proposed approach helps manufacturers improve their products based on user feedback.},
   author = {J Sangeetha and U Kumaran},
   doi = {10.1016/j.measen.2022.100619},
   issn = {26659174},
   journal = {Measurement: Sensors},
   keywords = {Harris hawks optimization,Product reviews Taylor series,RNN-BiLSTM,Sentiment analysis},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {A hybrid optimization algorithm using BiLSTM structure for sentiment analysis},
   volume = {25},
   year = {2023},
}
