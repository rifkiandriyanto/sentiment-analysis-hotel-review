@article{Japkowicz2002,
   author = {Nathalie Japkowicz and Shaju Stephen},
   issue = {5},
   journal = {Intelligent data analysis},
   pages = {429-449},
   publisher = {IOS Press},
   title = {The class imbalance problem: A systematic study},
   volume = {6},
   year = {2002},
}
@inproceedings{Kohavi1995,
   author = {Ron Kohavi and others},
   issue = {2},
   journal = {Ijcai},
   pages = {1137-1145},
   title = {A study of cross-validation and bootstrap for accuracy estimation and model selection},
   volume = {14},
   year = {1995},
}
@article{Wu2021,
   abstract = {We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.},
   author = {Peng Wu and Xiaotong Li and Chen Ling and Shengchun Ding and Si Shen},
   doi = {10.1016/J.ASOC.2021.107792},
   issn = {1568-4946},
   journal = {Applied Soft Computing},
   keywords = {Attention mechanism,Bidirectional long short-term memory network,Sentiment classification,Social media,Word embedding},
   month = {11},
   pages = {107792},
   publisher = {Elsevier},
   title = {Sentiment classification using attention mechanism and bidirectional long short-term memory network},
   volume = {112},
   year = {2021},
}
@article{Schuster1997,
   author = {Mike Schuster and Kuldip K Paliwal},
   issue = {11},
   journal = {IEEE transactions on Signal Processing},
   pages = {2673-2681},
   publisher = {Ieee},
   title = {Bidirectional recurrent neural networks},
   volume = {45},
   year = {1997},
}
@article{Chawla2002,
   author = {Nitesh V Chawla and Kevin W Bowyer and Lawrence O Hall and W Philip Kegelmeyer},
   journal = {Journal of artificial intelligence research},
   pages = {321-357},
   title = {SMOTE: synthetic minority over-sampling technique},
   volume = {16},
   year = {2002},
}
@article{Misra2020,
   author = {Puneet Misra and Arun Singh Yadav},
   issue = {3},
   journal = {Int. J. Emerg. Technol},
   pages = {659-665},
   title = {Improving the classification accuracy using recursive feature elimination with cross-validation},
   volume = {11},
   year = {2020},
}
@article{Fahrudin2019,
   author = {Tora Fahrudin and Joko Lianto Buliali and Chastine Fatichah},
   issue = {2},
   journal = {Int. J. Innov. Comput. Inf. Control},
   pages = {423-444},
   title = {Enhancing the performance of smote algorithm by using attribute weighting scheme and new selective sampling method for imbalanced data set},
   volume = {15},
   year = {2019},
}
@article{Graves2005,
   abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. © 2005 Elsevier Ltd. All rights reserved.},
   author = {Alex Graves and Jürgen Schmidhuber},
   doi = {10.1016/J.NEUNET.2005.06.042},
   issn = {0893-6080},
   issue = {5-6},
   journal = {Neural Networks},
   month = {7},
   pages = {602-610},
   pmid = {16112549},
   publisher = {Pergamon},
   title = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
   volume = {18},
   year = {2005},
}
@article{Li2022,
   abstract = {Sentiment analysis of online Chinese buzzwords (OCBs) is important for healthy development of platforms, such as games and social networking, which can avoid transmission of negative emotions through prediction of users’ sentiment tendencies. Buzzwords have the characteristics of varying text length, irregular wording, ignoring syntactic and grammatical requirements, no complete semantic structure, and no obvious sentiment features. This results in interference and challenges to the sentiment analysis of such texts. Sentiment analysis also requires capturing effective sentiment features from deeper contextual information. To solve the above problems, we propose a deep learning model combining BERT and BiLSTM. The goal is to generate dynamic representations of OCB vectors in downstream tasks by fine-tuning the BERT model and to capture the rich information of the text at the embedding layer to solve the problem of static representations of word vectors. The generated word vectors are then transferred to the BiLSTM model for feature extraction to obtain the local and global semantic features of the text while highlighting the text sentiment polarity for sentiment classification. The experimental results show that the model works well in terms of the comprehensive evaluation index F1. Our model also has important significance and research value for sentiment analysis of irregular texts, such as OCBs.},
   author = {Xinlu Li and Yuanyuan Lei and Shengwei Ji},
   doi = {10.3390/fi14110332},
   issn = {19995903},
   issue = {11},
   journal = {Future Internet},
   keywords = {BiLSTM,deep learning,online Chinese buzzwords,pre-trained language models,sentiment analysis},
   month = {11},
   publisher = {MDPI},
   title = {BERT- and BiLSTM-Based Sentiment Analysis of Online Chinese Buzzwords},
   volume = {14},
   year = {2022},
}
@article{Pei2022,
   abstract = {In recent years, more and more attention has been paid to text sentiment analysis, which has gradually become a research hotspot in information extraction, data mining, Natural Language Processing (NLP), and other fields. With the gradual popularization of the Internet, sentiment analysis of Uyghur texts has great research and application value in online public opinion. For low-resource languages, most state-of-the-art systems require tens of thousands of annotated sentences to get high performance. However, there is minimal annotated data available about Uyghur sentiment analysis tasks. There are also specificities in each task—differences in words and word order across languages make it a challenging problem. In this paper, we present an effective solution to providing a meaningful and easy-to-use feature extractor for sentiment analysis tasks: using the pre-trained language model with BiLSTM layer. Firstly, data augmentation is carried out by AEDA (An Easier Data Augmentation), and the augmented dataset is constructed to improve the performance of text classification tasks. Then, a pretraining model LaBSE is used to encode the input data. Then, BiLSTM is used to learn more context information. Finally, the validity of the model is verified via two categories datasets for sentiment analysis and five categories datasets for emotion analysis. We evaluated our approach on two datasets, which showed wonderful performance compared to some strong baselines. We close with an overview of the resources for sentiment analysis tasks and some of the open research questions. Therefore, we propose a combined deep learning and cross-language pretraining model for two low resource expectations.},
   author = {Yijie Pei and Siqi Chen and Zunwang Ke and Wushour Silamu and Qinglang Guo},
   doi = {10.3390/app12031182},
   issn = {20763417},
   issue = {3},
   journal = {Applied Sciences (Switzerland)},
   keywords = {BiLSTM,Cross-lingual pre-trained language model,Data augmentation,Low-resource,Sentiment analysis},
   month = {2},
   publisher = {MDPI},
   title = {AB-LaBSE: Uyghur Sentiment Analysis via the Pre-Training Model with BiLSTM},
   volume = {12},
   year = {2022},
}
@article{,
   abstract = {Subjectivity analysis is one of the key tasks in the field of natural language processing. Used to annotate data as subjective or objective, subjectivity analysis can be implemented on its own or as a precursor to other NLP applications such as sentiment analysis, emotion analysis, consumer review analysis, political opinion analysis, document summarization, and question answering systems. The main objective of this article is to test and compare six deep learning methods for subjectivity classification, including Long Short-Term Memory Networks (LSTM), Gated Recurrent Units (GRU), bidirectional GRU, bidirectional LSTM, LSTM with attention, and bidirectional LSTM with attention. We introduced a combination method for subjectivity annotation using lexicon-based and syntactic pattern-based methods. We evaluated the performance of GloVe versus one-hot encoding. We also reformatted, preprocessed, and annotated a political and ideological debate dataset for use in subjectivity analysis. Our research compares favorably with the performance of existing research on subjectivity analysis, achieving very high accuracy and evaluation metrics. LSTM with attention performed the best out of all the methods we tested with an accuracy of 97.39%.},
   author = {Ahmed Al Hamoud and Amber Hoenig and Kaushik Roy},
   doi = {10.1016/j.jksuci.2022.07.014},
   issn = {22131248},
   issue = {10},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   keywords = {Attention network,Deep learning,Gated Recurrent Unit (GRU),LSTM with attention,Long Short-Term Memory (LSTM),Subjectivity analysis},
   month = {11},
   pages = {7974-7987},
   publisher = {King Saud bin Abdulaziz University},
   title = {Sentence subjectivity analysis of a political and ideological debate dataset using LSTM and BiLSTM with attention and GRU models},
   volume = {34},
   year = {2022},
}
@article{Sangeetha2023,
   abstract = {Sentiment analysis can assist consumers in providing clear and objective sentiment recommendations based on large amounts of data, and it is helpful in overcoming unclear human flaws in subjective assessments. Existing sentiment analysis methods, on the other hand, must be enhanced in terms of robustness and accuracy. To improve marketing strategies based on product reviews, a reliable mechanism for forecasting sentiment polarity should be implemented. This paper proposes a new approach for sentiment analysis called Taylor–Harris Hawks Optimization driven long short-term memory (THHO- BiLSTM). By incorporating Taylor series in HHO, Taylor–HHO is formed, which aids in improving the BiLSTM classifier's performance by picking optimal weights in the hidden layers. The proposed method was evaluated using Amazon product reviews and reviews from the Taboada corpus benchmark datasets, yielding findings with 96.93% and 93% accuracy, respectively. When compared to existing approaches, the suggested model exceeds them in terms of accuracy. The proposed approach helps manufacturers improve their products based on user feedback.},
   author = {J. Sangeetha and U. Kumaran},
   doi = {10.1016/j.measen.2022.100619},
   issn = {26659174},
   journal = {Measurement: Sensors},
   keywords = {Harris hawks optimization,Product reviews Taylor series,RNN-BiLSTM,Sentiment analysis},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {A hybrid optimization algorithm using BiLSTM structure for sentiment analysis},
   volume = {25},
   year = {2023},
}
@article{Yuan2023,
   abstract = {Product reviews provide crucial information for both consumers and businesses, offering insights needed before purchasing a product or service. However, existing sentiment analysis methods, especially for Chinese language, struggle to effectively capture contextual information due to the complex semantics, multiple sentiment polarities, and long-term dependencies between words. In this paper, we propose a sentiment classification method based on the BiLSTM algorithm to address these challenges in natural language processing. Self-Attention-CNN BiLSTM (SAC-BiLSTM) leverages dual channels to extract features from both character-level embeddings and word-level embeddings. It combines BiLSTM and Self-Attention mechanisms for feature extraction and weight allocation, aiming to overcome the limitations in mining contextual information. Experiments were conducted on the onlineshopping10cats dataset, which is a standard corpus of e-commerce shopping reviews available in the ChineseNlpCorpus 2018. The experimental results demonstrate the effectiveness of our proposed algorithm, with Recall, Precision, and F1 scores reaching 0.9409, 0.9369, and 0.9404, respectively.},
   author = {Ye Yuan and Wang Wang and Guangze Wen and Zikun Zheng and Zhemin Zhuang},
   doi = {10.3390/fi15110364},
   issn = {19995903},
   issue = {11},
   journal = {Future Internet},
   keywords = {BiLSTM,natural language processing,scalable multi-channel,self-attention,sentiment classification},
   month = {11},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Sentiment Analysis of Chinese Product Reviews Based on Fusion of DUAL-Channel BiLSTM and Self-Attention},
   volume = {15},
   year = {2023},
}
@article{Chandio2022,
   abstract = {Deep neural networks have emerged as a leading approach towards handling many natural language processing (NLP) tasks. Deep networks initially conquered the problems of computer vision. However, dealing with sequential data such as text and sound was a nightmare for such networks as traditional deep networks are not reliable in preserving contextual information. This may not harm the results in the case of image processing where we do not care about the sequence, but when we consider the data collected from text for processing, such networks may trigger disastrous results. Moreover, establishing sentence semantics in a colloquial text such as Roman Urdu is a challenge. Additionally, the sparsity and high dimensionality of data in such informal text have encountered a significant challenge for building sentence semantics. To overcome this problem, we propose a deep recurrent architecture RU-BiLSTM based on bidirectional LSTM (BiLSTM) coupled with word embedding and an attention mechanism for sentiment analysis of Roman Urdu. Our proposed model uses the bidirectional LSTM to preserve the context in both directions and the attention mechanism to concentrate on more important features. Eventually, the last dense softmax output layer is used to acquire the binary and ternary classification results. We empirically evaluated our model on two available datasets of Roman Urdu, i.e., RUECD and RUSA-19. Our proposed model outperformed the baseline models on many grounds, and a significant improvement of 6% to 8% is achieved over baseline models.},
   author = {Bilal Ahmed Chandio and Ali Shariq Imran and Maheen Bakhtyar and Sher Muhammad Daudpota and Junaid Baber},
   doi = {10.3390/app12073641},
   issn = {20763417},
   issue = {7},
   journal = {Applied Sciences (Switzerland)},
   keywords = {LSTM,Roman Urdu,attention networks,deep learning,natural language processing,neural networks,sentiment analysis,text processing},
   month = {4},
   publisher = {MDPI},
   title = {Attention-Based RU-BiLSTM Sentiment Analysis Model for Roman Urdu},
   volume = {12},
   year = {2022},
}
@article{Bojanowski2016,
   abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
   author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
   month = {7},
   title = {Enriching Word Vectors with Subword Information},
   year = {2016},
}
@article{japkowicz2002class,
  title={The class imbalance problem: A systematic study},
  author={Japkowicz, Nathalie and Stephen, Shaju},
  journal={Intelligent data analysis},
  volume={6},
  number={5},
  pages={429--449},
  year={2002},
  publisher={IOS Press}
}
@book{liu2022sentiment,
  title={Sentiment analysis and opinion mining},
  author={Liu, Bing},
  year={2022},
  publisher={Springer Nature}
}
@book{jurafsky2000speech,
  title={Speech \& language processing},
  author={Jurafsky, Dan},
  year={2000},
  publisher={Pearson Education India}
}
@inproceedings{morin2005hierarchical,
  title={Hierarchical probabilistic neural network language model},
  author={Morin, Frederic and Bengio, Yoshua},
  booktitle={International workshop on artificial intelligence and statistics},
  pages={246--252},
  year={2005},
  organization={PMLR}
}
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{gunawan2021multilabel,
  title={Multilabel text classification menggunakan svm dan doc2vec classification pada dokumen berita bahasa indonesia},
  author={Gunawan, Kristian Indradiarta and Santoso, Joan},
  journal={Journal of Information System, Graphics, Hospitality and Technology},
  volume={3},
  number={01},
  pages={29--38},
  year={2021}
}
@article{yu2019review,
  title={A review of recurrent neural networks: LSTM cells and network architectures},
  author={Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  journal={Neural computation},
  volume={31},
  number={7},
  pages={1235--1270},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}