{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04666f49ac27d4c1e6e0aab6fe295245bcc7b22e"
   },
   "source": [
    "# Starter Code for fastText English Word Vectors Embedding\n",
    "\n",
    "This kernel intends to be a starter code for anyone using the fastText Embedding. It uses Gensim to create a `KeyedVector` object (behavior similar to a dictionary). An example of tokenizing the data is also given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "20ccefecab3bca5c5f0dccdfe5befd2e663b5250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quora-insincere-questions-classification', 'fasttext-wikinews']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../input'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "FILE_PATH = '../input/fasttext-wikinews/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "249b30328d7aa9effd7d7c42373c20481d8a3e12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 300\n",
      "\n",
      ", 0.1073 0.0089 0.0006 0.0055 -0.0646 -0.0600 0.0450 -0.0133 -0.0357 0.0430 -0.0\n",
      "the 0.0897 0.0160 -0.0571 0.0405 -0.0696 -0.1237 0.0301 0.0248 -0.0303 0.0174 0.\n",
      ". 0.0004 0.0032 -0.0204 0.0479 -0.0450 -0.1165 0.0142 0.0068 -0.0334 -0.0504 0.0\n",
      "and -0.0314 0.0149 -0.0205 0.0557 0.0205 -0.0405 0.0044 -0.0118 -0.0424 -0.0490 \n"
     ]
    }
   ],
   "source": [
    "# Let's read the first few lines \n",
    "with open(FILE_PATH) as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline()[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5fe6da8b88a544bb2a3055657f9fc5ddad76f2ff"
   },
   "source": [
    "## Load the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "52ead221bd6a5058f2496a0f3b2d4eb878975cee"
   },
   "outputs": [],
   "source": [
    "# This may take a few mins\n",
    "keyed_vec = KeyedVectors.load_word2vec_format(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f829dfaa3f63cb4bfa73bb2ed3b25d290ef10cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello is in the vocabulary: True\n",
      "! is in the vocabulary: True\n",
      "2 is in the vocabulary: True\n",
      "Turing is in the vocabulary: True\n",
      "foobarz is in the vocabulary: False\n",
      "hi! is in the vocabulary: False\n"
     ]
    }
   ],
   "source": [
    "for word in ['hello', '!', '2', 'Turing', 'foobarz', 'hi!']:\n",
    "    print(word, \"is in the vocabulary:\", word in keyed_vec.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10c20e26f6e2eb85877d0f735560e3e7e6b6fa27"
   },
   "source": [
    "### Retrieving a vector with the KeyedVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1d9810361256a46147609c779562afbf85dcf8c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.1366  0.0041  0.0905  0.0684 -0.0082  0.0175 -0.1518  0.1521  0.2366\n",
      " -0.1034  0.2554 -0.1798 -0.0465  0.2005 -0.1291  0.0709 -0.258  -0.212\n",
      " -0.0824  0.0465 -0.4044 -0.2766  0.004   0.3014  0.0622]\n"
     ]
    }
   ],
   "source": [
    "word_vec = keyed_vec.get_vector('foobar')\n",
    "print(word_vec.shape)\n",
    "print(word_vec[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "180c439dcf858ca56393acad576eface1dc0233a"
   },
   "source": [
    "### Creating Keras Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6f7dc3d679463988f9145aebf92ad86a288144fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'embedding_1',\n",
       " 'trainable': False,\n",
       " 'batch_input_shape': (None, None),\n",
       " 'dtype': 'float32',\n",
       " 'input_dim': 1000000,\n",
       " 'output_dim': 300,\n",
       " 'embeddings_initializer': {'class_name': 'RandomUniform',\n",
       "  'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}},\n",
       " 'embeddings_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'embeddings_constraint': None,\n",
       " 'mask_zero': False,\n",
       " 'input_length': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_embedding = keyed_vec.get_keras_embedding()\n",
    "keras_embedding.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c783d32b0472d79b8c55ccceb01cc9294258411"
   },
   "source": [
    "## Applied Example: Prediction with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2fe738fcac46daf4679f3c5ba52d4c6fcbbf19ca"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4648711227b86ee9bccab27728c5c1ba16048052"
   },
   "outputs": [],
   "source": [
    "def mean_fasttext(arr, embedding_dim=300):\n",
    "    '''\n",
    "    Create the average of the fasttext embeddings from each word in a document. \n",
    "    Very slow function, needs to be optimized for larger datasets\n",
    "    '''\n",
    "    mean_vectors = []\n",
    "    for document in arr:\n",
    "        tokens = nltk.tokenize.word_tokenize(document)\n",
    "        vectors = [keyed_vec.get_vector(token) for token in tokens if token in keyed_vec.vocab]\n",
    "        if vectors:\n",
    "            mean_vec = np.vstack(vectors).mean(axis=0)\n",
    "            mean_vectors.append(mean_vec)\n",
    "        else:\n",
    "            mean_vectors.append(np.zeros(embedding_dim))\n",
    "    embedding = np.vstack(mean_vectors)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f5b987750cdbee6e69337f31130d9878edba8212"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', nrows=6000)\n",
    "train_sample = data_sample[:5000]\n",
    "test_sample = data_sample[5000:]\n",
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "23667b8d0a81ee0271b29f3735de71687659d953",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = mean_fasttext(train_sample[\"question_text\"].values)\n",
    "X_test = mean_fasttext(test_sample[\"question_text\"].values)\n",
    "y_train = train_sample['target'].values\n",
    "y_test = test_sample['target'].values\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2f8f02185142f7d894c9818f5fe8cb79e3e46a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.0755813953488372\n",
      "Test Score: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train Score:\", f1_score(y_train, model.predict(X_train)))\n",
    "print(\"Test Score:\", f1_score(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
