{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8531859,"sourceType":"datasetVersion","datasetId":5095690}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom gensim.models import Word2Vec, KeyedVectors\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport multiprocessing\nfrom time import time\n\n# Mount Google Drive to access files\n# drive.mount('/content/drive')\n\n# Load dataset\ndata_path = '/kaggle/input/tripadvisor-1000-dataset-examples/trip-advisor-copy.csv'\n# data_path = '/content/drive/My Drive/experiment/trip-advisor.csv'\ndata = pd.read_csv(data_path)\n\n# Preprocessing: Text Cleaning, Tokenization, and Normalization\nnltk.download('punkt')\n\ndef clean_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text.lower()\n\ndata['Cleaned_Review'] = data['Review'].apply(clean_text)\n\n# Adjust labels for sentiment analysis\ndata['Sentiment'] = pd.cut(data['Rating'], bins=[-np.inf, 2, 3, np.inf], labels=['negative', 'neutral', 'positive'], right=False)\n\n# Split data into train and test sets (70% train, 30% test)\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    data['Cleaned_Review'], data['Sentiment'], test_size=0.3, random_state=42)\n\n# Print total sentiment in train and test set\nprint(\"\\nTotal sentiments in train set:\")\nprint(train_labels.value_counts())\n\nprint(\"\\nTotal sentiments in test set:\")\nprint(test_labels.value_counts())\n\n# Tokenize the text for Word2Vec\nsentences = [nltk.word_tokenize(text) for text in data['Cleaned_Review']]\n\n# Train Word2Vec model\ncores = multiprocessing.cpu_count()\nw2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     vector_size=300,\n                     sample=6e-5,\n                     alpha=0.03,\n                     min_alpha=0.0007,\n                     negative=20,\n                     workers=cores-1)\n\nt = time()\nw2v_model.build_vocab(sentences, progress_per=10000)\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n\nt = time()\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\nprint('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n\n# Convert text data to Word2Vec vectors (average of word vectors)\ndef text_to_vector(text):\n    tokens = nltk.word_tokenize(text.lower())\n    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n\ntrain_vectors = np.array([text_to_vector(text) for text in train_texts])\ntest_vectors = np.array([text_to_vector(text) for text in test_texts])\n\nclass SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.linear = nn.Linear(300, 768)\n        self.dropout = nn.Dropout(p=0.3)\n        self.classifier = nn.Linear(768 * 2, n_classes)  # Combine BERT and Word2Vec embeddings\n\n    def forward(self, input_ids, attention_mask, word2vec_embeddings):\n        # BERT\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = bert_outputs.pooler_output\n\n        # Word2Vec\n        word2vec_embeddings = self.linear(word2vec_embeddings)\n        word2vec_embeddings = self.dropout(word2vec_embeddings)\n\n        # Combine BERT and Word2Vec embeddings\n        combined = torch.cat((pooled_output, word2vec_embeddings), dim=1)\n        output = self.classifier(combined)\n        return output\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentimentClassifier(len(data['Sentiment'].unique()))\nmodel = model.to(device)\n\nclass ReviewDataset(Dataset):\n    def __init__(self, reviews, ratings):\n        self.reviews = reviews\n        self.ratings = ratings\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = self.reviews.iloc[idx]\n        rating = self.ratings.iloc[idx]\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=512,\n            return_token_type_ids=False,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        word2vec_vector = torch.tensor(text_to_vector(review), dtype=torch.float)\n\n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'word2vec_embeddings': word2vec_vector,\n            'labels': torch.tensor(0 if rating == 'negative' else 1 if rating == 'neutral' else 2, dtype=torch.long)  # Adjusted labels\n        }\n\ndef create_data_loader(reviews, ratings, batch_size):\n    ds = ReviewDataset(reviews, ratings)\n    return DataLoader(ds, batch_size=batch_size, num_workers=2)\n\nBATCH_SIZE = 64\ntrain_data_loader = create_data_loader(train_texts, train_labels, BATCH_SIZE)\ntest_data_loader = create_data_loader(test_texts, test_labels, BATCH_SIZE)\n\n# Define optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n# Training loop\nEPOCHS = 10\n\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        word2vec_embeddings = d[\"word2vec_embeddings\"].to(device)\n        labels = d[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, word2vec_embeddings=word2vec_embeddings)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, labels)\n\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# Main training loop with epoch print statements\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        None,\n        len(train_texts)\n    )\n\n    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {train_loss}, Accuracy: {train_acc}')\n\ndef get_predictions(model, data_loader):\n    model = model.eval()\n    predictions = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            word2vec_embeddings = d[\"word2vec_embeddings\"].to(device)\n            labels = d[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, word2vec_embeddings=word2vec_embeddings)\n            _, preds = torch.max(outputs, dim=1)\n\n            predictions.extend(preds)\n            real_values.extend(labels)\n\n    return torch.stack(predictions).cpu(), torch.stack(real_values).cpu()\n\ny_pred, y_true = get_predictions(model, test_data_loader)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\nprint(conf_matrix)\nprint(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n\n# Visualize Confusion Matrix\nlabels = ['negative', 'neutral', 'positive']\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T17:02:36.209681Z","iopub.execute_input":"2024-05-27T17:02:36.210058Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\nTotal sentiments in train set:\nSentiment\npositive    560\nnegative     72\nneutral      67\nName: count, dtype: int64\n\nTotal sentiments in test set:\nSentiment\npositive    221\nneutral      41\nnegative     38\nName: count, dtype: int64\nTime to build vocab: 0.0 mins\nTime to train the model: 0.02 mins\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aa59237154348658c7e879cebf2e325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2210687745e84533a0385877a278e0af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a790356eea4b15a2f316543fa317e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93ffb7ff82ab43b1be740225f6010616"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd0051455e44369bcd025fe33991cdc"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]}]}