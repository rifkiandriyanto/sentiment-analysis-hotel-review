{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1CkONyLzIQW",
        "outputId": "d87e6d77-bcb6-43d6-b269-e8463f33efe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total sentiments in train set:\n",
            "Sentiment\n",
            "positive    560\n",
            "negative     72\n",
            "neutral      67\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total sentiments in test set:\n",
            "Sentiment\n",
            "positive    221\n",
            "neutral      41\n",
            "negative     38\n",
            "Name: count, dtype: int64\n",
            "Time to build vocab: 0.0 mins\n",
            "Time to train the model: 0.15 mins\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch 1/1\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "# Crashed one\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import re\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import multiprocessing\n",
        "from time import time\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "data_path = '/content/drive/My Drive/experiment/trip-advisor-copy.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Preprocessing: Text Cleaning, Tokenization, and Normalization\n",
        "nltk.download('punkt')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.lower()\n",
        "\n",
        "data['Cleaned_Review'] = data['Review'].apply(clean_text)\n",
        "\n",
        "# Adjust labels for sentiment analysis\n",
        "data['Sentiment'] = pd.cut(data['Rating'], bins=[-np.inf, 2, 3, np.inf], labels=['negative', 'neutral', 'positive'], right=False)\n",
        "\n",
        "# Split data into train and test sets (70% train, 30% test)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['Cleaned_Review'], data['Sentiment'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Print total sentiment in train and test set\n",
        "print(\"\\nTotal sentiments in train set:\")\n",
        "print(train_labels.value_counts())\n",
        "\n",
        "print(\"\\nTotal sentiments in test set:\")\n",
        "print(test_labels.value_counts())\n",
        "\n",
        "# Convert text data to TaggedDocuments for Doc2Vec\n",
        "documents = [TaggedDocument(nltk.word_tokenize(text.lower()), [i]) for i, text in enumerate(data['Cleaned_Review'])]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "cores = multiprocessing.cpu_count()\n",
        "d2v_model = Doc2Vec(vector_size=300,\n",
        "                    window=2,\n",
        "                    min_count=2,\n",
        "                    workers=cores-1,\n",
        "                    epochs=40)\n",
        "\n",
        "t = time()\n",
        "d2v_model.build_vocab(documents)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "d2v_model.train(documents, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "# Convert text data to Doc2Vec vectors\n",
        "def text_to_vector(text):\n",
        "    return d2v_model.infer_vector(nltk.word_tokenize(text.lower()))\n",
        "\n",
        "train_vectors = np.array([text_to_vector(text) for text in train_texts])\n",
        "test_vectors = np.array([text_to_vector(text) for text in test_texts])\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = nn.Linear(300, 768)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.classifier = nn.Linear(768 * 2, n_classes)  # Combine BERT and Doc2Vec embeddings\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, doc2vec_embeddings):\n",
        "        # BERT\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_outputs.pooler_output\n",
        "\n",
        "        # Doc2Vec\n",
        "        doc2vec_embeddings = self.linear(doc2vec_embeddings)\n",
        "        doc2vec_embeddings = self.dropout(doc2vec_embeddings)\n",
        "\n",
        "        # Combine BERT and Doc2Vec embeddings\n",
        "        combined = torch.cat((pooled_output, doc2vec_embeddings), dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentimentClassifier(len(data['Sentiment'].unique()))\n",
        "model = model.to(device)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, ratings):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = self.reviews.iloc[idx]\n",
        "        rating = self.ratings.iloc[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        doc2vec_vector = torch.tensor(text_to_vector(review), dtype=torch.float)\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'doc2vec_embeddings': doc2vec_vector,\n",
        "            'labels': torch.tensor(0 if rating == 'negative' else 1 if rating == 'neutral' else 2, dtype=torch.long)  # Adjusted labels\n",
        "        }\n",
        "\n",
        "def create_data_loader(reviews, ratings, batch_size):\n",
        "    ds = ReviewDataset(reviews, ratings)\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "train_data_loader = create_data_loader(train_texts, train_labels, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_texts, test_labels, BATCH_SIZE)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 1\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        doc2vec_embeddings = d[\"doc2vec_embeddings\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, doc2vec_embeddings=doc2vec_embeddings)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f'Epoch {1}/{EPOCHS}')\n",
        "print('-' * 10)\n",
        "\n",
        "train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    None,\n",
        "    len(train_texts)\n",
        ")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            doc2vec_embeddings = d[\"doc2vec_embeddings\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, doc2vec_embeddings=doc2vec_embeddings)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            real_values.extend(labels)\n",
        "\n",
        "    return torch.stack(predictions).cpu(), torch.stack(real_values).cpu()\n",
        "\n",
        "y_pred, y_true = get_predictions(model, test_data_loader)\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(conf_matrix)\n",
        "print(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
