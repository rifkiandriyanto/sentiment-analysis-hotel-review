{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06592c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:15:55.603950Z",
     "iopub.status.busy": "2024-12-29T11:15:55.603664Z",
     "iopub.status.idle": "2024-12-29T11:16:00.559907Z",
     "shell.execute_reply": "2024-12-29T11:16:00.558447Z"
    },
    "papermill": {
     "duration": 4.962107,
     "end_time": "2024-12-29T11:16:00.562254",
     "exception": false,
     "start_time": "2024-12-29T11:15:55.600147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PySastrawi\r\n",
      "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl.metadata (892 bytes)\r\n",
      "Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: PySastrawi\r\n",
      "Successfully installed PySastrawi-1.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install PySastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c39dd2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:16:00.568487Z",
     "iopub.status.busy": "2024-12-29T11:16:00.568130Z",
     "iopub.status.idle": "2024-12-29T11:46:31.062209Z",
     "shell.execute_reply": "2024-12-29T11:46:31.061293Z"
    },
    "papermill": {
     "duration": 1830.49857,
     "end_time": "2024-12-29T11:46:31.063710",
     "exception": false,
     "start_time": "2024-12-29T11:16:00.565140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "Dataset loaded successfully!\n",
      "\n",
      "Head of dataset before preprocessing:\n",
      "        userName  score                   at  \\\n",
      "0   arum diajeng      3  2023-12-29 11:12:17   \n",
      "1    Iga Dwiyana      5  2023-12-29 09:56:34   \n",
      "2    Abdul Fatah      1  2023-12-29 09:44:19   \n",
      "3  Erdo Prasetya      5  2023-12-29 08:21:18   \n",
      "4    ilham yusuf      4  2023-12-29 08:04:46   \n",
      "\n",
      "                                             content  \n",
      "0  saya kemarin udah tes psi, tes kesehatan dan b...  \n",
      "1              Cepat sekali 3hari sudah diterima sim  \n",
      "2  Ini gimana ,kok sulit sekali untuk verifikasi ...  \n",
      "3  Mantap sih dr test sampai diterima simnya hany...  \n",
      "4  Untuk pembuatan sim baru secara online belum b...   \n",
      "\n",
      "\n",
      "Preprocessing complete in 1805.04 seconds!\n",
      "\n",
      "Head of dataset after preprocessing:\n",
      "        userName  score                   at  \\\n",
      "0   arum diajeng      3  2023-12-29 11:12:17   \n",
      "1    Iga Dwiyana      5  2023-12-29 09:56:34   \n",
      "2    Abdul Fatah      1  2023-12-29 09:44:19   \n",
      "3  Erdo Prasetya      5  2023-12-29 08:21:18   \n",
      "4    ilham yusuf      4  2023-12-29 08:04:46   \n",
      "\n",
      "                                             content     label  \\\n",
      "0  saya kemarin udah tes psi, tes kesehatan dan b...   neutral   \n",
      "1              Cepat sekali 3hari sudah diterima sim  positive   \n",
      "2  Ini gimana ,kok sulit sekali untuk verifikasi ...  negative   \n",
      "3  Mantap sih dr test sampai diterima simnya hany...  positive   \n",
      "4  Untuk pembuatan sim baru secara online belum b...  positive   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  kemarin udah tes psi tes sehat bayar status te...  \n",
      "1                                   cepat terima sim  \n",
      "2  gimana sulit verifikasi e ktp nya foto selfi w...  \n",
      "3  mantap sih dr test terima simnya butuh hr kena...  \n",
      "4                                 buat sim online ya   \n",
      "\n",
      "\n",
      "Word2Vec model trained successfully!\n",
      "\n",
      "\n",
      "Word2Vec embedding complete!\n",
      "\n",
      "\n",
      "Data before SMOTE oversampling:\n",
      "X_train shape: (8000, 300), y_train distribution:\n",
      "label\n",
      "negative    3756\n",
      "positive    3350\n",
      "neutral      894\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "SMOTE complete!\n",
      "\n",
      "Data after SMOTE oversampling:\n",
      "X_train_smote shape: (11268, 300), y_train_smote distribution:\n",
      "label\n",
      "negative    3756\n",
      "positive    3756\n",
      "neutral     3756\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# General Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import multiprocessing\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# === Define Device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load Dataset ===\n",
    "data_path = \"/kaggle/input/korlantas-polri-application-reviews/ulasan-korlantas.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "print(\"\\nDataset loaded successfully!\\n\")\n",
    "print(\"Head of dataset before preprocessing:\")\n",
    "print(data.head(), \"\\n\")  # Show dataset head before preprocessing\n",
    "data.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "# === Sentiment Mapping ===\n",
    "def rating_to_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'negative'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "data['label'] = data['score'].apply(rating_to_sentiment)\n",
    "\n",
    "# === Initialize Stemmer ===\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# === Stopwords Setup ===\n",
    "try:\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "except OSError:\n",
    "    print(\"Indonesian stopwords not found in NLTK, defaulting to English stopwords.\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# === Preprocessing Function ===\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)  # Remove non-alphabet characters\n",
    "        text = text.lower()  # Lowercase the text\n",
    "        tokens = nltk.word_tokenize(text)  # Tokenize the text\n",
    "        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "\n",
    "        # Apply stemming using multiprocessing\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "            tokens = pool.map(stemmer.stem, tokens)\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing text: {text}\\nException: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "start_time = time.time()\n",
    "data['cleaned_content'] = data['content'].apply(preprocess_text)\n",
    "print(f\"\\nPreprocessing complete in {time.time() - start_time:.2f} seconds!\\n\")\n",
    "print(\"Head of dataset after preprocessing:\")\n",
    "print(data.head(), \"\\n\")  # Show dataset head after preprocessing\n",
    "\n",
    "# === Word2Vec Embedding ===\n",
    "sentences = [nltk.word_tokenize(row) for row in data['cleaned_content'] if row]\n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=5, min_count=5, workers=multiprocessing.cpu_count())\n",
    "w2v_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "print(\"\\nWord2Vec model trained successfully!\\n\")\n",
    "\n",
    "def text_to_vector(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "\n",
    "# Convert text to vectors\n",
    "data['vectors'] = data['cleaned_content'].apply(text_to_vector)\n",
    "print(\"\\nWord2Vec embedding complete!\\n\")\n",
    "\n",
    "# === Split Dataset ===\n",
    "X = np.array(data['vectors'].tolist())\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# === SMOTE Oversampling ===\n",
    "print(\"\\nData before SMOTE oversampling:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train distribution:\\n{y_train.value_counts()}\\n\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nSMOTE complete!\\n\")\n",
    "print(\"Data after SMOTE oversampling:\")\n",
    "print(f\"X_train_smote shape: {X_train_smote.shape}, y_train_smote distribution:\\n{y_train_smote.value_counts()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7285345a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:46:31.071085Z",
     "iopub.status.busy": "2024-12-29T11:46:31.070739Z",
     "iopub.status.idle": "2024-12-29T11:48:56.322649Z",
     "shell.execute_reply": "2024-12-29T11:48:56.321599Z"
    },
    "papermill": {
     "duration": 145.257319,
     "end_time": "2024-12-29T11:48:56.324198",
     "exception": false,
     "start_time": "2024-12-29T11:46:31.066879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Machine Learning Models ===\n",
      "\n",
      "\n",
      "Training SVM...\n",
      "\n",
      "Train Accuracy for SVM: 0.6653\n",
      "Train Log Loss for SVM: 0.7676\n",
      "Validation Accuracy for SVM: 0.6625\n",
      "Validation Log Loss for SVM: 0.7438\n",
      "\n",
      "Confusion Matrix for SVM:\n",
      "\n",
      "[[579 295  65]\n",
      " [ 65 126  33]\n",
      " [ 57 160 620]]\n",
      "\n",
      "Classification Report for SVM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71       939\n",
      "     neutral       0.22      0.56      0.31       224\n",
      "    positive       0.86      0.74      0.80       837\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.64      0.64      0.61      2000\n",
      "weighted avg       0.77      0.66      0.70      2000\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Train Accuracy for Random Forest: 0.9996\n",
      "Train Log Loss for Random Forest: 0.1436\n",
      "Validation Accuracy for Random Forest: 0.7280\n",
      "Validation Log Loss for Random Forest: 0.6930\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "\n",
      "[[757 100  82]\n",
      " [133  39  52]\n",
      " [118  59 660]]\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.81      0.78       939\n",
      "     neutral       0.20      0.17      0.18       224\n",
      "    positive       0.83      0.79      0.81       837\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.59      0.59      0.59      2000\n",
      "weighted avg       0.72      0.73      0.72      2000\n",
      "\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Train Accuracy for Logistic Regression: 0.6579\n",
      "Train Log Loss for Logistic Regression: 0.7669\n",
      "Validation Accuracy for Logistic Regression: 0.6835\n",
      "Validation Log Loss for Logistic Regression: 0.7416\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "\n",
      "[[607 252  80]\n",
      " [ 70 110  44]\n",
      " [ 52 135 650]]\n",
      "\n",
      "Classification Report for Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.65      0.73       939\n",
      "     neutral       0.22      0.49      0.31       224\n",
      "    positive       0.84      0.78      0.81       837\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.63      0.64      0.61      2000\n",
      "weighted avg       0.77      0.68      0.71      2000\n",
      "\n",
      "\n",
      "Training LightGBM...\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 11268, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Train Accuracy for LightGBM: 0.9783\n",
      "Train Log Loss for LightGBM: 0.2260\n",
      "Validation Accuracy for LightGBM: 0.7195\n",
      "Validation Log Loss for LightGBM: 0.6608\n",
      "\n",
      "Confusion Matrix for LightGBM:\n",
      "\n",
      "[[724 138  77]\n",
      " [113  60  51]\n",
      " [115  67 655]]\n",
      "\n",
      "Classification Report for LightGBM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.77       939\n",
      "     neutral       0.23      0.27      0.25       224\n",
      "    positive       0.84      0.78      0.81       837\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.61      0.61      0.61      2000\n",
      "weighted avg       0.73      0.72      0.73      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Machine Learning Models ===\n",
    "def train_ml_models(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'LightGBM': lgb.LGBMClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name}...\\n\")\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Train evaluation\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_loss = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_train_pred_proba = model.predict_proba(X_train)\n",
    "            train_loss = log_loss(y_train, y_train_pred_proba, labels=['negative', 'neutral', 'positive'])\n",
    "\n",
    "        # Validation (Test) evaluation\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_loss = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_test_pred_proba = model.predict_proba(X_test)\n",
    "            test_loss = log_loss(y_test, y_test_pred_proba, labels=['negative', 'neutral', 'positive'])\n",
    "\n",
    "        print(f\"Train Accuracy for {model_name}: {train_accuracy:.4f}\")\n",
    "        if train_loss is not None:\n",
    "            print(f\"Train Log Loss for {model_name}: {train_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Train Log Loss for {model_name}: Not available (no predict_proba method)\")\n",
    "\n",
    "        print(f\"Validation Accuracy for {model_name}: {test_accuracy:.4f}\")\n",
    "        if test_loss is not None:\n",
    "            print(f\"Validation Log Loss for {model_name}: {test_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Validation Log Loss for {model_name}: Not available (no predict_proba method)\")\n",
    "\n",
    "        # Additional metrics (confusion matrix and classification report)\n",
    "        cm = confusion_matrix(y_test, y_test_pred, labels=['negative', 'neutral', 'positive'])\n",
    "        print(f\"\\nConfusion Matrix for {model_name}:\\n\")\n",
    "        print(cm)\n",
    "\n",
    "        report = classification_report(y_test, y_test_pred, target_names=['negative', 'neutral', 'positive'])\n",
    "        print(f\"\\nClassification Report for {model_name}:\\n\")\n",
    "        print(report)\n",
    "\n",
    "        # Save results\n",
    "        results[model_name] = {\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'train_loss': train_loss,\n",
    "            'validation_accuracy': test_accuracy,\n",
    "            'validation_loss': test_loss,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': report\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"\\n=== Training Machine Learning Models ===\\n\")\n",
    "ml_results = train_ml_models(X_train_smote, y_train_smote, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8d8379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:48:56.331525Z",
     "iopub.status.busy": "2024-12-29T11:48:56.331241Z",
     "iopub.status.idle": "2024-12-29T11:48:56.546490Z",
     "shell.execute_reply": "2024-12-29T11:48:56.545573Z"
    },
    "papermill": {
     "duration": 0.220485,
     "end_time": "2024-12-29T11:48:56.548057",
     "exception": false,
     "start_time": "2024-12-29T11:48:56.327572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Unified Hyperparameter Configuration ===\n",
    "hyperparams = {\n",
    "    \"input_size\": 300,        # Input vector size\n",
    "    \"hidden_size\": 256,       # Hidden size for LSTM\n",
    "    \"num_filters\": 256,       # Number of filters for CNN\n",
    "    \"kernel_sizes\": [3],      # Kernel sizes for CNN\n",
    "    \"hidden_units\": 256,      # Fully connected layer units for CNN\n",
    "    \"output_size\": len(data['label'].unique()),  # Number of output classes\n",
    "    \"dropout\": 0.3,           # Dropout rate\n",
    "    \"learning_rate\": 1e-3,    # Learning rate\n",
    "    \"batch_size\": 64,         # Batch size for DataLoader\n",
    "    \"num_epochs\": 50,         # Number of epochs for training\n",
    "    \"num_layers\": 3           # Number of layers for LSTM\n",
    "}\n",
    "\n",
    "# === Convert Data to Tensors ===\n",
    "X_train_tensor = torch.tensor(X_train_smote, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(pd.Categorical(y_train_smote).codes, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(pd.Categorical(y_test).codes, dtype=torch.long).to(device)\n",
    "\n",
    "# === DataLoader ===\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd77fc4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T11:48:56.555553Z",
     "iopub.status.busy": "2024-12-29T11:48:56.555256Z",
     "iopub.status.idle": "2024-12-29T11:49:57.831681Z",
     "shell.execute_reply": "2024-12-29T11:49:57.830778Z"
    },
    "papermill": {
     "duration": 61.281612,
     "end_time": "2024-12-29T11:49:57.832989",
     "exception": false,
     "start_time": "2024-12-29T11:48:56.551377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LSTM Model ===\n",
      "\n",
      "Epoch 1/50:\n",
      "    Train Loss: 0.8473, Train Accuracy: 0.5982\n",
      "    Val Loss: 0.7582, Val Accuracy: 0.6145\n",
      "Epoch 2/50:\n",
      "    Train Loss: 0.7833, Train Accuracy: 0.6387\n",
      "    Val Loss: 0.7546, Val Accuracy: 0.6140\n",
      "Epoch 3/50:\n",
      "    Train Loss: 0.7726, Train Accuracy: 0.6451\n",
      "    Val Loss: 0.7773, Val Accuracy: 0.5990\n",
      "Epoch 4/50:\n",
      "    Train Loss: 0.7628, Train Accuracy: 0.6514\n",
      "    Val Loss: 0.6929, Val Accuracy: 0.6770\n",
      "Epoch 5/50:\n",
      "    Train Loss: 0.7537, Train Accuracy: 0.6544\n",
      "    Val Loss: 0.7069, Val Accuracy: 0.6610\n",
      "Epoch 6/50:\n",
      "    Train Loss: 0.7491, Train Accuracy: 0.6597\n",
      "    Val Loss: 0.7185, Val Accuracy: 0.6530\n",
      "Epoch 7/50:\n",
      "    Train Loss: 0.7369, Train Accuracy: 0.6636\n",
      "    Val Loss: 0.7039, Val Accuracy: 0.6655\n",
      "Epoch 8/50:\n",
      "    Train Loss: 0.7293, Train Accuracy: 0.6668\n",
      "    Val Loss: 0.7306, Val Accuracy: 0.6425\n",
      "Epoch 9/50:\n",
      "    Train Loss: 0.7231, Train Accuracy: 0.6766\n",
      "    Val Loss: 0.6830, Val Accuracy: 0.6885\n",
      "Epoch 10/50:\n",
      "    Train Loss: 0.7238, Train Accuracy: 0.6782\n",
      "    Val Loss: 0.7396, Val Accuracy: 0.6320\n",
      "Epoch 11/50:\n",
      "    Train Loss: 0.6982, Train Accuracy: 0.6906\n",
      "    Val Loss: 0.7193, Val Accuracy: 0.6550\n",
      "Epoch 12/50:\n",
      "    Train Loss: 0.6823, Train Accuracy: 0.6942\n",
      "    Val Loss: 0.7259, Val Accuracy: 0.6480\n",
      "Epoch 13/50:\n",
      "    Train Loss: 0.6852, Train Accuracy: 0.6940\n",
      "    Val Loss: 0.7284, Val Accuracy: 0.6455\n",
      "Epoch 14/50:\n",
      "    Train Loss: 0.6843, Train Accuracy: 0.6988\n",
      "    Val Loss: 0.7221, Val Accuracy: 0.6510\n",
      "Epoch 15/50:\n",
      "    Train Loss: 0.6793, Train Accuracy: 0.6983\n",
      "    Val Loss: 0.7112, Val Accuracy: 0.6640\n",
      "Epoch 16/50:\n",
      "    Train Loss: 0.6764, Train Accuracy: 0.6986\n",
      "    Val Loss: 0.7219, Val Accuracy: 0.6535\n",
      "Epoch 17/50:\n",
      "    Train Loss: 0.6774, Train Accuracy: 0.7032\n",
      "    Val Loss: 0.7168, Val Accuracy: 0.6600\n",
      "Epoch 18/50:\n",
      "    Train Loss: 0.6742, Train Accuracy: 0.7018\n",
      "    Val Loss: 0.7263, Val Accuracy: 0.6505\n",
      "Epoch 19/50:\n",
      "    Train Loss: 0.6722, Train Accuracy: 0.7029\n",
      "    Val Loss: 0.7284, Val Accuracy: 0.6500\n",
      "Epoch 20/50:\n",
      "    Train Loss: 0.6715, Train Accuracy: 0.7066\n",
      "    Val Loss: 0.7231, Val Accuracy: 0.6570\n",
      "Epoch 21/50:\n",
      "    Train Loss: 0.6679, Train Accuracy: 0.7051\n",
      "    Val Loss: 0.7247, Val Accuracy: 0.6565\n",
      "Epoch 22/50:\n",
      "    Train Loss: 0.6686, Train Accuracy: 0.7062\n",
      "    Val Loss: 0.7270, Val Accuracy: 0.6535\n",
      "Epoch 23/50:\n",
      "    Train Loss: 0.6659, Train Accuracy: 0.7048\n",
      "    Val Loss: 0.7267, Val Accuracy: 0.6545\n",
      "Epoch 24/50:\n",
      "    Train Loss: 0.6671, Train Accuracy: 0.7053\n",
      "    Val Loss: 0.7269, Val Accuracy: 0.6545\n",
      "Epoch 25/50:\n",
      "    Train Loss: 0.6689, Train Accuracy: 0.7060\n",
      "    Val Loss: 0.7270, Val Accuracy: 0.6550\n",
      "Epoch 26/50:\n",
      "    Train Loss: 0.6659, Train Accuracy: 0.7056\n",
      "    Val Loss: 0.7278, Val Accuracy: 0.6540\n",
      "Epoch 27/50:\n",
      "    Train Loss: 0.6677, Train Accuracy: 0.7043\n",
      "    Val Loss: 0.7261, Val Accuracy: 0.6560\n",
      "Epoch 28/50:\n",
      "    Train Loss: 0.6682, Train Accuracy: 0.7050\n",
      "    Val Loss: 0.7261, Val Accuracy: 0.6565\n",
      "Epoch 29/50:\n",
      "    Train Loss: 0.6628, Train Accuracy: 0.7088\n",
      "    Val Loss: 0.7260, Val Accuracy: 0.6570\n",
      "Epoch 30/50:\n",
      "    Train Loss: 0.6642, Train Accuracy: 0.7064\n",
      "    Val Loss: 0.7265, Val Accuracy: 0.6565\n",
      "Epoch 31/50:\n",
      "    Train Loss: 0.6627, Train Accuracy: 0.7055\n",
      "    Val Loss: 0.7264, Val Accuracy: 0.6570\n",
      "Epoch 32/50:\n",
      "    Train Loss: 0.6643, Train Accuracy: 0.7054\n",
      "    Val Loss: 0.7264, Val Accuracy: 0.6570\n",
      "Epoch 33/50:\n",
      "    Train Loss: 0.6615, Train Accuracy: 0.7076\n",
      "    Val Loss: 0.7264, Val Accuracy: 0.6570\n",
      "Epoch 34/50:\n",
      "    Train Loss: 0.6654, Train Accuracy: 0.7038\n",
      "    Val Loss: 0.7264, Val Accuracy: 0.6565\n",
      "Epoch 35/50:\n",
      "    Train Loss: 0.6654, Train Accuracy: 0.7062\n",
      "    Val Loss: 0.7264, Val Accuracy: 0.6570\n",
      "Epoch 36/50:\n",
      "    Train Loss: 0.6637, Train Accuracy: 0.7054\n",
      "    Val Loss: 0.7265, Val Accuracy: 0.6570\n",
      "Epoch 37/50:\n",
      "    Train Loss: 0.6703, Train Accuracy: 0.7065\n",
      "    Val Loss: 0.7265, Val Accuracy: 0.6565\n",
      "Epoch 38/50:\n",
      "    Train Loss: 0.6657, Train Accuracy: 0.7082\n",
      "    Val Loss: 0.7265, Val Accuracy: 0.6565\n",
      "Epoch 39/50:\n",
      "    Train Loss: 0.6678, Train Accuracy: 0.7084\n",
      "    Val Loss: 0.7267, Val Accuracy: 0.6565\n",
      "Epoch 40/50:\n",
      "    Train Loss: 0.6647, Train Accuracy: 0.7032\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 41/50:\n",
      "    Train Loss: 0.6675, Train Accuracy: 0.7023\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 42/50:\n",
      "    Train Loss: 0.6668, Train Accuracy: 0.7058\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 43/50:\n",
      "    Train Loss: 0.6697, Train Accuracy: 0.7039\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 44/50:\n",
      "    Train Loss: 0.6632, Train Accuracy: 0.7061\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 45/50:\n",
      "    Train Loss: 0.6631, Train Accuracy: 0.7078\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 46/50:\n",
      "    Train Loss: 0.6654, Train Accuracy: 0.7087\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 47/50:\n",
      "    Train Loss: 0.6619, Train Accuracy: 0.7042\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 48/50:\n",
      "    Train Loss: 0.6689, Train Accuracy: 0.7061\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 49/50:\n",
      "    Train Loss: 0.6652, Train Accuracy: 0.7086\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "Epoch 50/50:\n",
      "    Train Loss: 0.6641, Train Accuracy: 0.7080\n",
      "    Val Loss: 0.7266, Val Accuracy: 0.6560\n",
      "\n",
      "=== Evaluating LSTM Model ===\n",
      "\n",
      "LSTM Test Accuracy: 0.6560\n",
      "Confusion Matrix:\n",
      "[[579 304  56]\n",
      " [ 78 116  30]\n",
      " [ 61 159 617]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.62      0.70       939\n",
      "     neutral       0.20      0.52      0.29       224\n",
      "    positive       0.88      0.74      0.80       837\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.63      0.62      0.60      2000\n",
      "weighted avg       0.77      0.66      0.70      2000\n",
      "\n",
      "\n",
      "=== Training CNN Model ===\n",
      "\n",
      "Epoch 1/50:\n",
      "    Train Loss: 0.8218, Train Accuracy: 0.6196\n",
      "    Val Loss: 0.7350, Val Accuracy: 0.6425\n",
      "Epoch 2/50:\n",
      "    Train Loss: 0.7790, Train Accuracy: 0.6460\n",
      "    Val Loss: 0.7316, Val Accuracy: 0.6420\n",
      "Epoch 3/50:\n",
      "    Train Loss: 0.7591, Train Accuracy: 0.6557\n",
      "    Val Loss: 0.7483, Val Accuracy: 0.6395\n",
      "Epoch 4/50:\n",
      "    Train Loss: 0.7390, Train Accuracy: 0.6719\n",
      "    Val Loss: 0.7379, Val Accuracy: 0.6435\n",
      "Epoch 5/50:\n",
      "    Train Loss: 0.7222, Train Accuracy: 0.6773\n",
      "    Val Loss: 0.7434, Val Accuracy: 0.6450\n",
      "Epoch 6/50:\n",
      "    Train Loss: 0.6989, Train Accuracy: 0.6941\n",
      "    Val Loss: 0.7255, Val Accuracy: 0.6545\n",
      "Epoch 7/50:\n",
      "    Train Loss: 0.6812, Train Accuracy: 0.7051\n",
      "    Val Loss: 0.7008, Val Accuracy: 0.6875\n",
      "Epoch 8/50:\n",
      "    Train Loss: 0.6581, Train Accuracy: 0.7191\n",
      "    Val Loss: 0.7093, Val Accuracy: 0.6785\n",
      "Epoch 9/50:\n",
      "    Train Loss: 0.6317, Train Accuracy: 0.7306\n",
      "    Val Loss: 0.7834, Val Accuracy: 0.6475\n",
      "Epoch 10/50:\n",
      "    Train Loss: 0.6097, Train Accuracy: 0.7439\n",
      "    Val Loss: 0.7578, Val Accuracy: 0.6725\n",
      "Epoch 11/50:\n",
      "    Train Loss: 0.5818, Train Accuracy: 0.7576\n",
      "    Val Loss: 0.7679, Val Accuracy: 0.6720\n",
      "Epoch 12/50:\n",
      "    Train Loss: 0.5587, Train Accuracy: 0.7703\n",
      "    Val Loss: 0.7744, Val Accuracy: 0.6780\n",
      "Epoch 13/50:\n",
      "    Train Loss: 0.5359, Train Accuracy: 0.7792\n",
      "    Val Loss: 0.8005, Val Accuracy: 0.6705\n",
      "Epoch 14/50:\n",
      "    Train Loss: 0.5119, Train Accuracy: 0.7939\n",
      "    Val Loss: 0.7790, Val Accuracy: 0.7140\n",
      "Epoch 15/50:\n",
      "    Train Loss: 0.4901, Train Accuracy: 0.8035\n",
      "    Val Loss: 0.8146, Val Accuracy: 0.6795\n",
      "Epoch 16/50:\n",
      "    Train Loss: 0.4726, Train Accuracy: 0.8116\n",
      "    Val Loss: 0.8081, Val Accuracy: 0.7115\n",
      "Epoch 17/50:\n",
      "    Train Loss: 0.4483, Train Accuracy: 0.8222\n",
      "    Val Loss: 0.8184, Val Accuracy: 0.7060\n",
      "Epoch 18/50:\n",
      "    Train Loss: 0.4237, Train Accuracy: 0.8348\n",
      "    Val Loss: 0.8403, Val Accuracy: 0.7100\n",
      "Epoch 19/50:\n",
      "    Train Loss: 0.4149, Train Accuracy: 0.8356\n",
      "    Val Loss: 0.8906, Val Accuracy: 0.7090\n",
      "Epoch 20/50:\n",
      "    Train Loss: 0.4027, Train Accuracy: 0.8419\n",
      "    Val Loss: 0.8969, Val Accuracy: 0.7000\n",
      "Epoch 21/50:\n",
      "    Train Loss: 0.3742, Train Accuracy: 0.8549\n",
      "    Val Loss: 0.9483, Val Accuracy: 0.6830\n",
      "Epoch 22/50:\n",
      "    Train Loss: 0.3649, Train Accuracy: 0.8574\n",
      "    Val Loss: 0.9512, Val Accuracy: 0.7065\n",
      "Epoch 23/50:\n",
      "    Train Loss: 0.3353, Train Accuracy: 0.8693\n",
      "    Val Loss: 0.9746, Val Accuracy: 0.7015\n",
      "Epoch 24/50:\n",
      "    Train Loss: 0.3302, Train Accuracy: 0.8709\n",
      "    Val Loss: 1.0255, Val Accuracy: 0.7000\n",
      "Epoch 25/50:\n",
      "    Train Loss: 0.3152, Train Accuracy: 0.8788\n",
      "    Val Loss: 1.0281, Val Accuracy: 0.7060\n",
      "Epoch 26/50:\n",
      "    Train Loss: 0.2878, Train Accuracy: 0.8868\n",
      "    Val Loss: 1.1060, Val Accuracy: 0.6615\n",
      "Epoch 27/50:\n",
      "    Train Loss: 0.2930, Train Accuracy: 0.8860\n",
      "    Val Loss: 1.0959, Val Accuracy: 0.6950\n",
      "Epoch 28/50:\n",
      "    Train Loss: 0.2678, Train Accuracy: 0.8954\n",
      "    Val Loss: 1.1548, Val Accuracy: 0.6825\n",
      "Epoch 29/50:\n",
      "    Train Loss: 0.2592, Train Accuracy: 0.8996\n",
      "    Val Loss: 1.1482, Val Accuracy: 0.7115\n",
      "Epoch 30/50:\n",
      "    Train Loss: 0.2418, Train Accuracy: 0.9068\n",
      "    Val Loss: 1.1857, Val Accuracy: 0.6895\n",
      "Epoch 31/50:\n",
      "    Train Loss: 0.2511, Train Accuracy: 0.9001\n",
      "    Val Loss: 1.2215, Val Accuracy: 0.7085\n",
      "Epoch 32/50:\n",
      "    Train Loss: 0.2213, Train Accuracy: 0.9135\n",
      "    Val Loss: 1.2823, Val Accuracy: 0.7285\n",
      "Epoch 33/50:\n",
      "    Train Loss: 0.2348, Train Accuracy: 0.9095\n",
      "    Val Loss: 1.2520, Val Accuracy: 0.7050\n",
      "Epoch 34/50:\n",
      "    Train Loss: 0.2052, Train Accuracy: 0.9200\n",
      "    Val Loss: 1.3063, Val Accuracy: 0.6930\n",
      "Epoch 35/50:\n",
      "    Train Loss: 0.1972, Train Accuracy: 0.9236\n",
      "    Val Loss: 1.2833, Val Accuracy: 0.6950\n",
      "Epoch 36/50:\n",
      "    Train Loss: 0.1837, Train Accuracy: 0.9307\n",
      "    Val Loss: 1.3843, Val Accuracy: 0.6970\n",
      "Epoch 37/50:\n",
      "    Train Loss: 0.1793, Train Accuracy: 0.9285\n",
      "    Val Loss: 1.4739, Val Accuracy: 0.7100\n",
      "Epoch 38/50:\n",
      "    Train Loss: 0.1717, Train Accuracy: 0.9345\n",
      "    Val Loss: 1.4785, Val Accuracy: 0.7140\n",
      "Epoch 39/50:\n",
      "    Train Loss: 0.1713, Train Accuracy: 0.9364\n",
      "    Val Loss: 1.4656, Val Accuracy: 0.7080\n",
      "Epoch 40/50:\n",
      "    Train Loss: 0.1874, Train Accuracy: 0.9288\n",
      "    Val Loss: 1.4366, Val Accuracy: 0.7045\n",
      "Epoch 41/50:\n",
      "    Train Loss: 0.1452, Train Accuracy: 0.9466\n",
      "    Val Loss: 1.5092, Val Accuracy: 0.7020\n",
      "Epoch 42/50:\n",
      "    Train Loss: 0.1488, Train Accuracy: 0.9453\n",
      "    Val Loss: 1.6266, Val Accuracy: 0.6655\n",
      "Epoch 43/50:\n",
      "    Train Loss: 0.1428, Train Accuracy: 0.9450\n",
      "    Val Loss: 1.6103, Val Accuracy: 0.7055\n",
      "Epoch 44/50:\n",
      "    Train Loss: 0.1226, Train Accuracy: 0.9545\n",
      "    Val Loss: 1.6208, Val Accuracy: 0.7050\n",
      "Epoch 45/50:\n",
      "    Train Loss: 0.1290, Train Accuracy: 0.9515\n",
      "    Val Loss: 1.6426, Val Accuracy: 0.6980\n",
      "Epoch 46/50:\n",
      "    Train Loss: 0.1164, Train Accuracy: 0.9590\n",
      "    Val Loss: 1.6699, Val Accuracy: 0.7105\n",
      "Epoch 47/50:\n",
      "    Train Loss: 0.1131, Train Accuracy: 0.9584\n",
      "    Val Loss: 1.7236, Val Accuracy: 0.7160\n",
      "Epoch 48/50:\n",
      "    Train Loss: 0.1142, Train Accuracy: 0.9602\n",
      "    Val Loss: 1.7743, Val Accuracy: 0.6970\n",
      "Epoch 49/50:\n",
      "    Train Loss: 0.1055, Train Accuracy: 0.9617\n",
      "    Val Loss: 1.8150, Val Accuracy: 0.7170\n",
      "Epoch 50/50:\n",
      "    Train Loss: 0.1627, Train Accuracy: 0.9417\n",
      "    Val Loss: 1.7545, Val Accuracy: 0.7100\n",
      "\n",
      "=== Evaluating CNN Model ===\n",
      "\n",
      "CNN Test Accuracy: 0.7100\n",
      "Confusion Matrix:\n",
      "[[732 107 100]\n",
      " [126  43  55]\n",
      " [139  53 645]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.78      0.76       939\n",
      "     neutral       0.21      0.19      0.20       224\n",
      "    positive       0.81      0.77      0.79       837\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.58      0.58      0.58      2000\n",
      "weighted avg       0.71      0.71      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Define LSTM Model ===\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        output = self.fc(self.dropout(h_n[-1]))\n",
    "        return output\n",
    "\n",
    "# === Train LSTM Model ===\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # === Training Phase ===\n",
    "        model.train()\n",
    "        total_loss, correct = 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y_batch).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # === Validation Phase ===\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Print Epoch Results\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"    Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# === Evaluate LSTM Model ===\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predictions = outputs.argmax(1)\n",
    "        acc = (predictions == y_test).float().mean().item()\n",
    "        cm = confusion_matrix(y_test.cpu(), predictions.cpu())\n",
    "        cr = classification_report(y_test.cpu(), predictions.cpu(), target_names=['negative', 'neutral', 'positive'])\n",
    "    return acc, cm, cr\n",
    "\n",
    "# Initialize LSTM Model\n",
    "lstm_model = SentimentLSTM(\n",
    "    input_size=hyperparams[\"input_size\"],\n",
    "    hidden_size=hyperparams[\"hidden_size\"],\n",
    "    output_size=hyperparams[\"output_size\"],\n",
    "    num_layers=hyperparams[\"num_layers\"],\n",
    "    dropout=hyperparams[\"dropout\"]\n",
    ").to(device)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "lstm_loss_fn = nn.CrossEntropyLoss()\n",
    "lstm_scheduler = optim.lr_scheduler.StepLR(lstm_optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\n=== Training LSTM Model ===\\n\")\n",
    "train_model(lstm_model, train_loader, val_loader, lstm_optimizer, lstm_loss_fn, lstm_scheduler, hyperparams[\"num_epochs\"])\n",
    "\n",
    "# Evaluate LSTM\n",
    "print(\"\\n=== Evaluating LSTM Model ===\\n\")\n",
    "accuracy, confusion_mat, class_report = evaluate_model(lstm_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"LSTM Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "\n",
    "# === Define CNN Model ===\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_filters, kernel_sizes, hidden_units, output_size, dropout):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=input_size,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=k,\n",
    "                      padding=k // 2)  # Add padding to maintain dimensions\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(num_filters * len(kernel_sizes), hidden_units)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).permute(0, 2, 1)  # Reshape for CNN\n",
    "        convs_out = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = torch.relu(conv(x))\n",
    "            pooled_out = self.global_pool(conv_out).squeeze(2)\n",
    "            convs_out.append(pooled_out)\n",
    "\n",
    "        concatenated = torch.cat(convs_out, dim=1)\n",
    "        x = self.dropout1(torch.relu(self.fc1(concatenated)))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "# === Train CNN Model ===\n",
    "def train_cnn_model(model, train_loader, val_loader, optimizer, loss_fn, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # === Training Phase ===\n",
    "        model.train()\n",
    "        running_loss, correct_predictions = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct_predictions += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct_predictions / len(train_loader.dataset)\n",
    "\n",
    "        # === Validation Phase ===\n",
    "        model.eval()\n",
    "        val_running_loss, val_correct_predictions = 0.0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_correct_predictions += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct_predictions / len(val_loader.dataset)\n",
    "\n",
    "        # Print Epoch Results\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"    Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# === Evaluate CNN Model ===\n",
    "def evaluate_cnn(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "        cm = confusion_matrix(y_test_tensor.cpu(), predicted.cpu())\n",
    "        cr = classification_report(y_test_tensor.cpu(), predicted.cpu(), target_names=['negative', 'neutral', 'positive'])\n",
    "    return acc, cm, cr\n",
    "\n",
    "# Initialize CNN Model\n",
    "cnn_model = SentimentCNN(\n",
    "    input_size=hyperparams[\"input_size\"],\n",
    "    num_filters=hyperparams[\"num_filters\"],\n",
    "    kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "    hidden_units=hyperparams[\"hidden_units\"],\n",
    "    output_size=hyperparams[\"output_size\"],\n",
    "    dropout=hyperparams[\"dropout\"]\n",
    ").to(device)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "cnn_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\n=== Training CNN Model ===\\n\")\n",
    "train_cnn_model(cnn_model, train_loader, val_loader, cnn_optimizer, cnn_loss_fn, hyperparams[\"num_epochs\"])\n",
    "\n",
    "# Evaluate CNN\n",
    "print(\"\\n=== Evaluating CNN Model ===\\n\")\n",
    "cnn_acc, cnn_cm, cnn_cr = evaluate_cnn(cnn_model, X_test_tensor, y_test_tensor)\n",
    "print(f\"CNN Test Accuracy: {cnn_acc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cnn_cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cnn_cr)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6386919,
     "sourceId": 10316654,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2047.358113,
   "end_time": "2024-12-29T11:50:00.701055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-29T11:15:53.342942",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
